{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGPT (openai) sentiment analysis\n",
    "\n",
    "Data sources:\n",
    "1. Twitter: https://www.kaggle.com/code/paoloripamonti/twitter-sentiment-analysis/notebook\n",
    "2. IMDB: https://www.kaggle.com/code/lakshmi25npathi/sentiment-analysis-of-imdb-.movie-reviews/notebook\n",
    "3. Finance: https://www.kaggle.com/code/supreethrao/bert-s-a-stock-market-guru-86-22-huggingface/notebook\n",
    "4. Amazon: https://www.kaggle.com/code/anshulrai/cudnnlstm-implementation-93-7-accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "# from openai_module import do_stuff\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = open(\"key.txt\", \"r\").read()\n",
    "\n",
    "def do_stuff(prompt_order):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-002\",\n",
    "        prompt=prompt_order,\n",
    "        max_tokens=8,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    response_text = response.choices[0].text\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative' 'positive']\n",
      "['negative' 'positive']\n"
     ]
    }
   ],
   "source": [
    "# read in the twitter data\n",
    "twitter_data = pd.read_csv('data/twitter_data.csv', header=None, encoding= 'ISO-8859-1')\n",
    "twitter_data = twitter_data[[0,5]]\n",
    "twitter_data.columns = ['sentiment', 'text']\n",
    "twitter_data['sentiment'] = twitter_data['sentiment'].map({0: 'negative', 2: 'neutral', 4: 'positive'})\n",
    "\n",
    "# twitter has 2 classes: positive and negative\n",
    "print(twitter_data.sentiment.unique())\n",
    "\n",
    "# select a random sample of 300 rows from the twitter data\n",
    "twitter_data_sample = twitter_data.sample(n=300, random_state=101)\n",
    "print(twitter_data_sample.sentiment.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250977     21\n",
      "150064     22\n",
      "710275     24\n",
      "367641     26\n",
      "575674     21\n",
      "           ..\n",
      "223843      3\n",
      "926788      5\n",
      "1230705    13\n",
      "1008342    25\n",
      "206543     10\n",
      "Name: text, Length: 300, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# find the average number of words of the tweets\n",
    "print(twitter_data_sample.text.str.split().str.len().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [01:48<00:00,  2.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# run chatGPT on the sample\n",
    "twitter_sentiment = [do_stuff(f\"Classify the sentiment of the following text in one word (positive, negative): \\\"{tweet}\\\"\").strip().lower() for tweet in tqdm(twitter_data_sample['text'])]\n",
    "twitter_data_sample['predicted_sentiment'] = twitter_sentiment\n",
    "\n",
    "# reassign the sentiment values to 0, 1, 2\n",
    "# twitter_data_sample['sentiment'] = twitter_data_sample['sentiment'].map({'negative': 0, 'positive': 2})\n",
    "# twitter_data_sample['predicted_sentiment'] = twitter_data_sample['predicted_sentiment'].map({'negative': 0, 'positive': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7966666666666666\n",
      "[[  0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0]\n",
      " [  1   1 124  24   0   0]\n",
      " [  0   0  33 115   1   1]\n",
      " [  0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0]]\n",
      "                                        precision    recall  f1-score   support\n",
      "\n",
      "i have brought peace, freedom, justice       0.00      0.00      0.00         0\n",
      "                               jealous       0.00      0.00      0.00         0\n",
      "                              negative       0.79      0.83      0.81       150\n",
      "                              positive       0.83      0.77      0.80       150\n",
      "          the sentiment in the text is       0.00      0.00      0.00         0\n",
      "          the sentiment of the text is       0.00      0.00      0.00         0\n",
      "\n",
      "                              accuracy                           0.80       300\n",
      "                             macro avg       0.27      0.27      0.27       300\n",
      "                          weighted avg       0.81      0.80      0.80       300\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Desktop\\UPenn other stuff\\Penn Data Science Group\\openapi\\openapi-venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Admin\\Desktop\\UPenn other stuff\\Penn Data Science Group\\openapi\\openapi-venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Admin\\Desktop\\UPenn other stuff\\Penn Data Science Group\\openapi\\openapi-venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# calculate the accuracy of the predictions for Twitter\n",
    "print(accuracy_score(twitter_data_sample['sentiment'], twitter_data_sample['predicted_sentiment']))\n",
    "\n",
    "# print the confusion matrix\n",
    "print(confusion_matrix(twitter_data_sample['sentiment'], twitter_data_sample['predicted_sentiment']))\n",
    "\n",
    "# print the classification report\n",
    "print(classification_report(twitter_data_sample['sentiment'], twitter_data_sample['predicted_sentiment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive' 'negative']\n",
      "['positive' 'negative']\n"
     ]
    }
   ],
   "source": [
    "# read in the imdb data\n",
    "imdb_data = pd.read_csv('data/imdb_data.csv')\n",
    "\n",
    "# imdb has 2 classes: positive and negative\n",
    "print(imdb_data.sentiment.unique())\n",
    "\n",
    "# select a random sample of 300 rows from the imdb data\n",
    "imdb_data_sample = imdb_data.sample(n=300, random_state=102)\n",
    "print(imdb_data_sample.sentiment.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225.93\n"
     ]
    }
   ],
   "source": [
    "print(imdb_data_sample.review.str.split().str.len().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:04<00:00,  2.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# run chatGPT on the sample\n",
    "imdb_sentiment = [do_stuff(f\"Classify the sentiment of the following text in one word (positive, negative): \\\"{review}\\\"\").strip().lower() for review in tqdm(imdb_data_sample['review'])]\n",
    "imdb_data_sample['predicted_sentiment'] = imdb_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9166666666666666\n",
      "[[144   2   0]\n",
      " [ 22 131   1]\n",
      " [  0   0   0]]\n",
      "                              precision    recall  f1-score   support\n",
      "\n",
      "                    negative       0.87      0.99      0.92       146\n",
      "                    positive       0.98      0.85      0.91       154\n",
      "the sentiment of the text is       0.00      0.00      0.00         0\n",
      "\n",
      "                    accuracy                           0.92       300\n",
      "                   macro avg       0.62      0.61      0.61       300\n",
      "                weighted avg       0.93      0.92      0.92       300\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Desktop\\UPenn other stuff\\Penn Data Science Group\\openapi\\openapi-venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Admin\\Desktop\\UPenn other stuff\\Penn Data Science Group\\openapi\\openapi-venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Admin\\Desktop\\UPenn other stuff\\Penn Data Science Group\\openapi\\openapi-venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# calculate the accuracy of the predictions for IMDB\n",
    "print(accuracy_score(imdb_data_sample['sentiment'], imdb_data_sample['predicted_sentiment']))\n",
    "\n",
    "# print the confusion matrix\n",
    "print(confusion_matrix(imdb_data_sample['sentiment'], imdb_data_sample['predicted_sentiment']))\n",
    "\n",
    "# print the classification report\n",
    "print(classification_report(imdb_data_sample['sentiment'], imdb_data_sample['predicted_sentiment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive' 'negative' 'neutral']\n",
      "['neutral' 'positive' 'negative']\n"
     ]
    }
   ],
   "source": [
    "# read in the finance data\n",
    "finance_data = pd.read_csv('data/finance_data.csv')\n",
    "\n",
    "# finance data has 3 classes: positive, negative, and neutral\n",
    "print(finance_data.Sentiment.unique())\n",
    "\n",
    "# select a random sample of 300 rows from the finance data\n",
    "finance_data_sample = finance_data.sample(n=300, random_state=103)\n",
    "print(finance_data_sample.Sentiment.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Sentence', 'Sentiment'], dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finance_data_sample.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.826666666666668\n"
     ]
    }
   ],
   "source": [
    "print(finance_data_sample.Sentence.str.split().str.len().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [01:52<00:00,  2.67it/s]\n"
     ]
    }
   ],
   "source": [
    "# run chatGPT on the sample\n",
    "finance_sentiment = [do_stuff(f\"Classify the sentiment of the following text in one word (positive, negative, neutral): \\\"{sentence}\\\"\").strip().lower() for sentence in tqdm(finance_data_sample['Sentence'])]\n",
    "finance_data_sample['predicted_sentiment'] = finance_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6966666666666667\n",
      "[[ 35   1   1   0]\n",
      " [ 38 111  13   1]\n",
      " [  6  31  63   0]\n",
      " [  0   0   0   0]]\n",
      "                              precision    recall  f1-score   support\n",
      "\n",
      "                    negative       0.44      0.95      0.60        37\n",
      "                     neutral       0.78      0.68      0.73       163\n",
      "                    positive       0.82      0.63      0.71       100\n",
      "the sentiment in the text is       0.00      0.00      0.00         0\n",
      "\n",
      "                    accuracy                           0.70       300\n",
      "                   macro avg       0.51      0.56      0.51       300\n",
      "                weighted avg       0.75      0.70      0.71       300\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Desktop\\UPenn other stuff\\Penn Data Science Group\\openapi\\openapi-venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Admin\\Desktop\\UPenn other stuff\\Penn Data Science Group\\openapi\\openapi-venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Admin\\Desktop\\UPenn other stuff\\Penn Data Science Group\\openapi\\openapi-venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# calculate the accuracy of the predictions for finance data\n",
    "print(accuracy_score(finance_data_sample['Sentiment'], finance_data_sample['predicted_sentiment']))\n",
    "\n",
    "# print the confusion matrix\n",
    "print(confusion_matrix(finance_data_sample['Sentiment'], finance_data_sample['predicted_sentiment']))\n",
    "\n",
    "# print the classification report\n",
    "print(classification_report(finance_data_sample['Sentiment'], finance_data_sample['predicted_sentiment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1]\n",
      "['positive' 'negative']\n"
     ]
    }
   ],
   "source": [
    "# read in the amazon data\n",
    "amazon_data = pd.read_csv('data/amazon_data.csv', header=None)\n",
    "\n",
    "# finance data has 3 classes: positive and negative\n",
    "print(amazon_data[0].unique())\n",
    "\n",
    "# select a random sample of 300 rows from the amazon data\n",
    "amazon_data_sample = amazon_data.sample(n=300, random_state=104)\n",
    "\n",
    "# rename the columns\n",
    "amazon_data_sample.columns = ['sentiment', 'title', 'text']\n",
    "amazon_data_sample['sentiment'] = amazon_data_sample['sentiment'].map({2: 'positive', 1: 'negative'})\n",
    "print(amazon_data_sample.sentiment.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.17333333333333\n"
     ]
    }
   ],
   "source": [
    "print(amazon_data_sample.text.str.split().str.len().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [01:45<00:00,  2.85it/s]\n"
     ]
    }
   ],
   "source": [
    "# run chatGPT on the sample\n",
    "amazon_sentiment = [do_stuff(f\"Classify the sentiment of the following text in one word (positive, negative): \\\"{text}\\\"\").strip().lower() for text in tqdm(amazon_data_sample['text'])]\n",
    "amazon_data_sample['predicted_sentiment'] = amazon_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9133333333333333\n",
      "[[127   1   0   0]\n",
      " [  0   0   0   0]\n",
      " [ 19   4 147   2]\n",
      " [  0   0   0   0]]\n",
      "                              precision    recall  f1-score   support\n",
      "\n",
      "                    negative       0.87      0.99      0.93       128\n",
      "                     neutral       0.00      0.00      0.00         0\n",
      "                    positive       1.00      0.85      0.92       172\n",
      "the sentiment of the text is       0.00      0.00      0.00         0\n",
      "\n",
      "                    accuracy                           0.91       300\n",
      "                   macro avg       0.47      0.46      0.46       300\n",
      "                weighted avg       0.94      0.91      0.92       300\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Desktop\\UPenn other stuff\\Penn Data Science Group\\openapi\\openapi-venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Admin\\Desktop\\UPenn other stuff\\Penn Data Science Group\\openapi\\openapi-venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Admin\\Desktop\\UPenn other stuff\\Penn Data Science Group\\openapi\\openapi-venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# calculate the accuracy of the predictions for Amazon\n",
    "print(accuracy_score(amazon_data_sample['sentiment'], amazon_data_sample['predicted_sentiment']))\n",
    "\n",
    "# print the confusion matrix\n",
    "print(confusion_matrix(amazon_data_sample['sentiment'], amazon_data_sample['predicted_sentiment']))\n",
    "\n",
    "# print the classification report\n",
    "print(classification_report(amazon_data_sample['sentiment'], amazon_data_sample['predicted_sentiment']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openapi-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
